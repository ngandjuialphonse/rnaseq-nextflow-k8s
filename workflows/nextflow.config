/*
 * Nextflow Configuration File
 * ============================
 * 
 * This file defines:
 *   - Resource requirements for each process
 *   - Execution profiles (docker, kubernetes, test)
 *   - Container images
 *   - Kubernetes-specific settings
 * 
 * DEVOPS ANALOGY:
 *   Like application.yml in Spring Boot or docker-compose.yml
 *   Separates configuration from code
 */

/*
 * MANIFEST
 * ========
 * Pipeline metadata (like package.json)
 */
manifest {
    name            = 'RNA-Seq-Pipeline'
    author          = 'Bio-Informatic DevOps Engineer'
    homePage        = 'https://github.com/ngandjuialphonse/optimum-space-services'
    description     = 'Production-grade RNA-Seq analysis pipeline on Kubernetes'
    mainScript      = 'main.nf'
    nextflowVersion = '>=23.04.0'
    version         = '1.0.0'
}

/*
 * DEFAULT PARAMETERS
 * ==================
 * Can be overridden with --param_name value
 */
params {
    // Pipeline version
    version = '1.0.0'
    
    // Help flag
    help = false
    
    // Input/output
    reads = null
    genome = null
    gtf = null
    outdir = './results'
    
    // Resource parameters
    max_cpus = 16
    max_memory = '64.GB'
    max_time = '24.h'
    
    // Tool-specific threads
    fastqc_threads = 2
    star_threads = 8
    salmon_threads = 4
}

/*
 * PROCESS RESOURCE DEFAULTS
 * ==========================
 * Default resources for all processes
 * Can be overridden per-process below
 */
process {
    // Default resources
    cpus = 1
    memory = '2.GB'
    time = '1.h'
    
    // Error handling
    errorStrategy = 'retry'
    maxRetries = 2
    
    // Container settings (will be set by profiles)
    container = null
}

/*
 * EXECUTION PROFILES
 * ==================
 * Different configurations for different environments
 * 
 * Usage:
 *   nextflow run main.nf -profile docker
 *   nextflow run main.nf -profile kubernetes
 *   nextflow run main.nf -profile test,docker
 */

profiles {
    /*
     * DOCKER PROFILE
     * ==============
     * Run locally using Docker
     * Good for: Development, testing, small datasets
     */
    docker {
        docker.enabled = true
        docker.runOptions = '-u $(id -u):$(id -g)'  // Run as current user
        
        // Container images for each tool
        process {
            withName: FASTQC {
                container = 'biocontainers/fastqc:v0.12.1-0'
                cpus = 2
                memory = '4.GB'
            }
            
            withName: STAR_INDEX {
                container = 'quay.io/biocontainers/star:2.7.10a--h9ee0642_0'
                cpus = 8
                memory = '32.GB'
                time = '2.h'
            }
            
            withName: STAR_ALIGN {
                container = 'quay.io/biocontainers/star:2.7.10a--h9ee0642_0'
                cpus = 8
                memory = '32.GB'
                time = '2.h'
            }
            
            withName: SALMON_QUANT {
                container = 'quay.io/biocontainers/subread:2.0.3--h7132678_0'  // featureCounts
                cpus = 4
                memory = '8.GB'
            }
            
            withName: MULTIQC {
                container = 'quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0'
                cpus = 1
                memory = '2.GB'
            }
        }
    }
    
    /*
     * KUBERNETES PROFILE
     * ==================
     * Run on Kubernetes cluster
     * Good for: Production, large datasets, parallel processing
     * 
     * DEVOPS NOTE:
     * This is where the magic happens - Nextflow dynamically creates
     * pods for each task, each with its own resource requirements
     */
    kubernetes {
        process.executor = 'k8s'
        
        // Kubernetes-specific settings
        k8s {
            // Namespace for pipeline execution
            namespace = 'bioinformatics'
            
            // ServiceAccount with permissions to create pods
            serviceAccount = 'nextflow-sa'
            
            // Shared storage for data
            storageClaimName = 'genomics-data-pvc'
            storageMountPath = '/data'
            storageSubPath = '/'
            
            // Pod settings
            pullPolicy = 'IfNotPresent'
            
            // Resource limits (prevent runaway pods)
            pod {
                // Node selector (optional - target specific node pools)
                // nodeSelector = 'workload=bioinformatics'
                
                // Tolerations (optional - for tainted nodes)
                // toleration 'key=spot', 'operator=Equal', 'value=true', 'effect=NoSchedule'
                
                // Security context
                securityContext {
                    runAsUser = 1000
                    runAsGroup = 1000
                    fsGroup = 1000
                }
            }
        }
        
        // Container images (use your own registry in production)
        process {
            withName: FASTQC {
                container = 'biocontainers/fastqc:v0.12.1-0'
                cpus = 2
                memory = '4.GB'
                
                // Kubernetes-specific pod annotations
                pod = [
                    [label: 'app', value: 'fastqc'],
                    [annotation: 'cost-center', value: 'bioinformatics']
                ]
            }
            
            withName: STAR_INDEX {
                container = 'quay.io/biocontainers/star:2.7.10a--h9ee0642_0'
                cpus = 8
                memory = '32.GB'
                time = '2.h'
                
                pod = [
                    [label: 'app', value: 'star-index'],
                    [annotation: 'cost-center', value: 'bioinformatics']
                ]
            }
            
            withName: STAR_ALIGN {
                container = 'quay.io/biocontainers/star:2.7.10a--h9ee0642_0'
                cpus = 8
                memory = '32.GB'
                time = '2.h'
                
                // DEVOPS NOTE:
                // This is the most resource-intensive step
                // Consider using spot instances for cost savings
                pod = [
                    [label: 'app', value: 'star-align'],
                    [annotation: 'cost-center', value: 'bioinformatics'],
                    // Uncomment to use spot instances (AWS)
                    // [nodeSelector: 'eks.amazonaws.com/capacityType', value: 'SPOT']
                ]
            }
            
            withName: SALMON_QUANT {
                container = 'quay.io/biocontainers/subread:2.0.3--h7132678_0'
                cpus = 4
                memory = '8.GB'
                
                pod = [
                    [label: 'app', value: 'salmon-quant'],
                    [annotation: 'cost-center', value: 'bioinformatics']
                ]
            }
            
            withName: MULTIQC {
                container = 'quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0'
                cpus = 1
                memory = '2.GB'
                
                pod = [
                    [label: 'app', value: 'multiqc'],
                    [annotation: 'cost-center', value: 'bioinformatics']
                ]
            }
        }
    }
    
    /*
     * TEST PROFILE
     * ============
     * Use small test dataset for quick validation
     * Good for: CI/CD, development, testing changes
     */
    test {
        params {
            // Small test dataset
            reads = "${projectDir}/../data/fastq/test_R{1,2}.fastq.gz"
            genome = "${projectDir}/../data/reference/test_genome.fa"
            gtf = "${projectDir}/../data/reference/test_genes.gtf"
            outdir = "${projectDir}/../data/test_results"
        }
        
        // Reduce resource requirements for testing
        process {
            withName: STAR_INDEX {
                cpus = 4
                memory = '16.GB'
            }
            
            withName: STAR_ALIGN {
                cpus = 4
                memory = '16.GB'
            }
        }
    }
    
    /*
     * LOCAL PROFILE
     * =============
     * Run without containers (tools installed locally)
     * Good for: HPC environments, quick tests
     */
    local {
        process.executor = 'local'
        
        // Assume tools are in PATH
        process {
            withName: FASTQC {
                cpus = 2
                memory = '4.GB'
            }
            
            withName: STAR_INDEX {
                cpus = 8
                memory = '32.GB'
            }
            
            withName: STAR_ALIGN {
                cpus = 8
                memory = '32.GB'
            }
            
            withName: SALMON_QUANT {
                cpus = 4
                memory = '8.GB'
            }
            
            withName: MULTIQC {
                cpus = 1
                memory = '2.GB'
            }
        }
    }
    
    /*
     * SLURM PROFILE
     * =============
     * Run on HPC cluster with SLURM scheduler
     * Good for: Academic environments, shared HPC resources
     */
    slurm {
        process.executor = 'slurm'
        process.queue = 'batch'
        
        // SLURM-specific settings
        process {
            clusterOptions = '--account=bioinformatics'
            
            withName: STAR_ALIGN {
                queue = 'highmem'  // Use high-memory queue
                clusterOptions = '--account=bioinformatics --partition=highmem'
            }
        }
    }
}

/*
 * EXECUTION REPORTS
 * =================
 * Generate execution reports for monitoring and debugging
 */
timeline {
    enabled = true
    file = "${params.outdir}/timeline.html"
}

report {
    enabled = true
    file = "${params.outdir}/report.html"
}

trace {
    enabled = true
    file = "${params.outdir}/trace.txt"
    fields = 'task_id,hash,native_id,process,tag,name,status,exit,module,container,cpus,time,disk,memory,attempt,submit,start,complete,duration,realtime,queue,%cpu,%mem,rss,vmem,peak_rss,peak_vmem,rchar,wchar,syscr,syscw,read_bytes,write_bytes'
}

dag {
    enabled = true
    file = "${params.outdir}/dag.html"
}

/*
 * RESOURCE LIMITS
 * ===============
 * Prevent processes from requesting more than available
 * 
 * DEVOPS NOTE:
 * These act as circuit breakers - if a process requests more than
 * max_cpus or max_memory, Nextflow will cap it at these values
 */
executor {
    // Queue size (max number of tasks submitted at once)
    queueSize = 20
    
    // Poll interval (how often to check for completed tasks)
    pollInterval = '5 sec'
    
    // Submit rate limit (prevent overwhelming the scheduler)
    submitRateLimit = '10 sec'
}

/*
 * CLEANUP
 * =======
 * Automatically clean up work directory on success
 */
cleanup = false  // Set to true to auto-delete work dir on success

/*
 * DEVOPS NOTES
 * ============
 * 
 * RESOURCE ALLOCATION STRATEGY:
 * 
 * 1. FastQC (2 CPU, 4 GB):
 *    - Lightweight tool
 *    - Can run many in parallel
 *    - Good candidate for spot instances
 * 
 * 2. STAR Alignment (8 CPU, 32 GB):
 *    - Most resource-intensive step
 *    - Genome index must fit in RAM (~30 GB)
 *    - Bottleneck of the pipeline
 *    - Consider:
 *      * Spot instances (70% cost savings)
 *      * Memory-optimized instances (r5, r6i)
 *      * Pre-built index to avoid rebuilding
 * 
 * 3. Salmon/featureCounts (4 CPU, 8 GB):
 *    - Moderate resource usage
 *    - Fast execution
 *    - Good for general-purpose instances
 * 
 * 4. MultiQC (1 CPU, 2 GB):
 *    - Minimal resources
 *    - Quick execution
 *    - Can run on smallest instance type
 * 
 * COST OPTIMIZATION:
 * 
 * 1. Use spot instances for all tasks (set in pod nodeSelector)
 * 2. Pre-build STAR index (avoid rebuilding every time)
 * 3. Use cluster autoscaler (scale to zero when idle)
 * 4. Set resource quotas (prevent runaway jobs)
 * 5. Use tiered storage (S3 for cold data, EFS for hot data)
 * 
 * KUBERNETES BEST PRACTICES:
 * 
 * 1. Use separate namespace (isolation)
 * 2. Set resource requests AND limits (prevent noisy neighbors)
 * 3. Use PodDisruptionBudgets (for production)
 * 4. Enable pod autoscaling (HPA) for head pod
 * 5. Use network policies (security)
 * 6. Set up monitoring (Prometheus + Grafana)
 * 7. Use init containers for data validation
 * 8. Set proper security contexts (non-root user)
 * 
 * TROUBLESHOOTING:
 * 
 * 1. Out of Memory (OOM):
 *    - Increase memory in process definition
 *    - Check if genome index fits in RAM
 *    - Use memory-optimized instances
 * 
 * 2. Pod Eviction:
 *    - Check resource quotas
 *    - Check node capacity
 *    - Use PriorityClasses
 * 
 * 3. Slow Execution:
 *    - Check if using spot instances (can be interrupted)
 *    - Check network latency to storage
 *    - Verify CPU allocation
 * 
 * 4. Permission Denied:
 *    - Check RBAC settings
 *    - Verify ServiceAccount permissions
 *    - Check PVC access modes
 */
